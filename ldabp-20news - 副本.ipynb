{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397a83dd",
   "metadata": {},
   "source": [
    "## LDA-BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9017ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题0进行分支\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前时间切片: 1\n",
      "目前主题数: 8\n",
      "一致性分数: nan\n",
      "主题4进行分支\n",
      "主题5进行分支\n",
      "主题6进行分支\n",
      "当前时间切片: 2\n",
      "目前主题数: 12\n",
      "一致性分数: 0.37153318538848795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前时间切片: 3\n",
      "目前主题数: 13\n",
      "一致性分数: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "d:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前时间切片: 4\n",
      "目前主题数: 14\n",
      "一致性分数: nan\n",
      "主题7进行分支\n",
      "当前时间切片: 5\n",
      "目前主题数: 16\n",
      "一致性分数: 0.4298116902031362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 165>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    201\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, time_slice_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(time_slices, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 203\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mldabp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_slice_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mLDABPModel.forward\u001b[1;34m(self, new_text, time_slice)\u001b[0m\n\u001b[0;32m    101\u001b[0m doc_threshold_low \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_text) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m140\u001b[39m  \u001b[38;5;66;03m# 主题保留的最小文档数\u001b[39;00m\n\u001b[0;32m    102\u001b[0m doc_threshold_branch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_text) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# 分支的最小文档数\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m doc_propabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m topic_classify, doc_classify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_topic_count(doc_propabilities)\n\u001b[0;32m    107\u001b[0m branch_vectors \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mLDABPModel.get_prob\u001b[1;34m(self, new_text)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_text):\n\u001b[1;32m---> 59\u001b[0m     text_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     similar \u001b[38;5;241m=\u001b[39m cosine_similarity(text_vec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_vectors)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_argmax(similar)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mLDABPModel.doc2vec\u001b[1;34m(texts, dictionary)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdoc2vec\u001b[39m(texts, dictionary):\n\u001b[1;32m---> 49\u001b[0m     vec_list \u001b[38;5;241m=\u001b[39m [LDABPModel\u001b[38;5;241m.\u001b[39mget_doc_vector(text, dictionary) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(vec_list)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdoc2vec\u001b[39m(texts, dictionary):\n\u001b[1;32m---> 49\u001b[0m     vec_list \u001b[38;5;241m=\u001b[39m [\u001b[43mLDABPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(vec_list)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mLDABPModel.get_doc_vector\u001b[1;34m(doc, dictionary)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_doc_vector\u001b[39m(doc, dictionary):\n\u001b[1;32m---> 41\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(dictionary\u001b[38;5;241m.\u001b[39mdoc2idx(doc))\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_counts)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class LDABPModel:\n",
    "    def __init__(self, initial_num_topics=6, threshold=0.35, iterations=300, random_state=1):\n",
    "        self.initial_num_topics = initial_num_topics\n",
    "        self.threshold = threshold\n",
    "        self.iterations = iterations\n",
    "        self.random_state = random_state\n",
    "        self.dictionary = None\n",
    "        self.lda = None\n",
    "        self.topic_vectors = None\n",
    "        self.topics = None\n",
    "\n",
    "    def prepare_dictionary(self, text_data_list):\n",
    "        self.dictionary = corpora.Dictionary(text_data_list[0])\n",
    "        for text_data in text_data_list[1:]:\n",
    "            self.dictionary.add_documents(text_data)\n",
    "        return self.dictionary\n",
    "\n",
    "    def train_initial_lda(self, corpus):\n",
    "        self.lda = models.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=self.dictionary,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            iterations=self.iterations,\n",
    "            num_topics=self.initial_num_topics,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.topic_vectors = self.lda.state.get_lambda()\n",
    "        self.topics = list(range(self.initial_num_topics))\n",
    "        return self.lda\n",
    "\n",
    "    @staticmethod\n",
    "    def get_doc_vector(doc, dictionary):\n",
    "        vec = np.zeros(len(dictionary))\n",
    "        word_counts = pd.Series(dictionary.doc2idx(doc)).value_counts().reset_index()\n",
    "        for i in range(len(word_counts)):\n",
    "            vec[word_counts.iloc[i, 0]] = word_counts.iloc[i, 1]\n",
    "        return vec\n",
    "\n",
    "    @staticmethod\n",
    "    def doc2vec(texts, dictionary):\n",
    "        vec_list = [LDABPModel.get_doc_vector(text, dictionary) for text in texts]\n",
    "        return np.array(vec_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def max_argmax(mat, axis=1):\n",
    "        max_v = np.max(mat, axis=axis)\n",
    "        max_idx = np.argmax(mat, axis=axis)\n",
    "        return [[max_idx[i], max_v[i]] for i in range(len(max_v))]\n",
    "\n",
    "    def get_prob(self, new_text):\n",
    "        text_vec = self.doc2vec(new_text, self.dictionary)\n",
    "        similar = cosine_similarity(text_vec, self.topic_vectors)\n",
    "        return self.max_argmax(similar)\n",
    "\n",
    "    def get_topic_count(self, doc_propabilities):\n",
    "        topic_classify = dict.fromkeys(range(len(self.topics)), 0)\n",
    "        doc_classify = []\n",
    "        \n",
    "        for idx, v in doc_propabilities:\n",
    "            if v > self.threshold and idx < len(self.topics):\n",
    "                topic_classify[idx] += 1\n",
    "                doc_classify.append(idx)\n",
    "            else: \n",
    "                topic_classify[len(self.topics)-1] += 1\n",
    "                doc_classify.append(len(self.topics)-1)\n",
    "        return topic_classify, doc_classify\n",
    "\n",
    "    def get_coherence(self, new_topic_vectors, new_text):\n",
    "        topic_word_num = 10\n",
    "        tmp = new_topic_vectors.argsort(axis=1)[:, -topic_word_num:]\n",
    "        topic_words = [[] for _ in range(tmp.shape[0])]\n",
    "        \n",
    "        for i in range(tmp.shape[0]):\n",
    "            for j in range(tmp.shape[1]):\n",
    "                try:\n",
    "                    topic_words[i].append(self.dictionary[tmp[i, j]])\n",
    "                except KeyError:\n",
    "                    continue  \n",
    "        \n",
    "        coherence = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            corpus=[self.dictionary.doc2bow(text) for text in new_text],\n",
    "            dictionary=self.dictionary,\n",
    "            texts=new_text,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        return coherence.get_coherence(), topic_words\n",
    "\n",
    "    def forward(self, new_text, time_slice):\n",
    "        current_topics = self.topics.copy()\n",
    "        current_topics.append(f'slice{time_slice}')\n",
    "        \n",
    "        doc_threshold_low = len(new_text) // 140  # 主题保留的最小文档数\n",
    "        doc_threshold_branch = len(new_text) // 4  # 分支的最小文档数\n",
    "        \n",
    "        doc_propabilities = self.get_prob(new_text)\n",
    "        topic_classify, doc_classify = self.get_topic_count(doc_propabilities)\n",
    "        \n",
    "        branch_vectors = []\n",
    "        delete_index = []\n",
    "        \n",
    "        for index in topic_classify:\n",
    "            if topic_classify[index] > doc_threshold_branch:\n",
    "                print(f'主题{index}进行分支')\n",
    "                delete_index.append(index)\n",
    "                current_topics.extend([f'{index}_{i}' for i in range(1, 3)])\n",
    "                \n",
    "                branch_docs = [new_text[i] for i, cls in enumerate(doc_classify) if cls == index]\n",
    "                branch_corpus = [self.dictionary.doc2bow(text) for text in branch_docs]\n",
    "                \n",
    "                branch_lda = models.LdaModel(\n",
    "                    corpus=branch_corpus,\n",
    "                    id2word=self.dictionary,\n",
    "                    alpha='auto',\n",
    "                    eta='auto',\n",
    "                    iterations=self.iterations,\n",
    "                    num_topics=2,\n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                branch_vectors.extend(branch_lda.state.get_lambda())\n",
    "        \n",
    "        new_topic_docs = [new_text[i] for i, cls in enumerate(doc_classify) if cls == len(self.topics)-1]\n",
    "        new_topic_corpus = [self.dictionary.doc2bow(text) for text in new_topic_docs]\n",
    "        \n",
    "        new_topic_lda = models.LdaModel(\n",
    "            corpus=new_topic_corpus,\n",
    "            id2word=self.dictionary,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            iterations=self.iterations,\n",
    "            num_topics=1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        new_topic_vector = new_topic_lda.state.get_lambda()[0]\n",
    "        \n",
    "        new_topic_vectors = []\n",
    "        for key in list(topic_classify.keys())[:-1]:\n",
    "            if key not in delete_index and topic_classify[key] >= doc_threshold_low:\n",
    "                new_topic_vectors.append(self.topic_vectors[key])\n",
    "        \n",
    "        new_topic_vectors.append(new_topic_vector)\n",
    "        new_topic_vectors.extend(branch_vectors)\n",
    "        new_topic_vectors = np.array(new_topic_vectors)\n",
    "        \n",
    "        self.topics = [v for i, v in enumerate(current_topics) if i not in delete_index]\n",
    "        \n",
    "        coherence_score, topic_words = self.get_coherence(new_topic_vectors, new_text)\n",
    "        self.topic_vectors = new_topic_vectors\n",
    "        \n",
    "        print(f'当前时间切片: {time_slice}')\n",
    "        print(f'目前主题数: {len(self.topics)}')\n",
    "        print(f'一致性分数: {coherence_score}')\n",
    "        \n",
    "        return len(self.topics), self.topics, topic_words, new_topic_vectors, coherence_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # text_col放具体的文本数据列名\n",
    "    def load_text_data(file_paths, text_col='Processed_Text_No_Stop_Words'):\n",
    "        \"\"\"加载文本数据并转换为分词列表\"\"\"\n",
    "        text_data_list = []\n",
    "        for path in file_paths:\n",
    "            df = pd.read_excel(path, engine='openpyxl')\n",
    "            text_data = df[text_col].apply(lambda x: x.split()).tolist()\n",
    "            text_data_list.append(text_data)\n",
    "        return text_data_list\n",
    "\n",
    "    # 替换为需要的文件路径\n",
    "    file_paths = [\n",
    "        '46-55processed_text_no_stop_words.xlsx',\n",
    "        '56-65processed_text_no_stop_words.xlsx',\n",
    "        '66-75processed_text_no_stop_words.xlsx',\n",
    "        '76-85processed_text_no_stop_words.xlsx',\n",
    "        '86-95processed_text_no_stop_words.xlsx',\n",
    "        '96-05processed_text_no_stop_words.xlsx',\n",
    "        '06-15processed_text_no_stop_words.xlsx',\n",
    "        '16-23processed_text_no_stop_words.xlsx'\n",
    "    ]\n",
    "    \n",
    "    all_text_data = load_text_data(file_paths)\n",
    "    initial_text = all_text_data[0]  \n",
    "    time_slices = all_text_data[1:]  \n",
    "    \n",
    "    # 训练模型\n",
    "    ldabp = LDABPModel(initial_num_topics=6, threshold=0.35)\n",
    "    ldabp.prepare_dictionary(all_text_data)\n",
    "    \n",
    "    # 构建初始语料库\n",
    "    initial_corpus = [ldabp.dictionary.doc2bow(text) for text in initial_text]\n",
    "    ldabp.train_initial_lda(initial_corpus)\n",
    "    \n",
    "    # 处理时间切片并生成一致性分数等结果\n",
    "    results = []\n",
    "    for i, time_slice_text in enumerate(time_slices, start=1):\n",
    "        res = ldabp.forward(time_slice_text, i)\n",
    "        results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc914f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m Dictionary(text)\n\u001b[0;32m     27\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(text)]\n\u001b[1;32m---> 28\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m coherence_model_lda \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mlda_model, texts\u001b[38;5;241m=\u001b[39mtext, dictionary\u001b[38;5;241m=\u001b[39mdictionary)\n\u001b[0;32m     30\u001b[0m coherence_score \u001b[38;5;241m=\u001b[39m coherence_model_lda\u001b[38;5;241m.\u001b[39mget_coherence()\n",
      "File \u001b[1;32md:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\models\\ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    525\u001b[0m )\n",
      "File \u001b[1;32md:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\models\\ldamodel.py:991\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    988\u001b[0m reallen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)  \u001b[38;5;66;03m# keep track of how many documents we've processed so far\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_every \u001b[38;5;129;01mand\u001b[39;00m ((reallen \u001b[38;5;241m==\u001b[39m lencorpus) \u001b[38;5;129;01mor\u001b[39;00m ((chunk_no \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (eval_every \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumworkers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlencorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;66;03m# add the chunk to dispatcher's job queue, so workers can munch on it\u001b[39;00m\n\u001b[0;32m    995\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, dispatching documents up to #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    997\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[0;32m    998\u001b[0m     )\n",
      "File \u001b[1;32md:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\models\\ldamodel.py:847\u001b[0m, in \u001b[0;36mLdaModel.log_perplexity\u001b[1;34m(self, chunk, total_docs)\u001b[0m\n\u001b[0;32m    845\u001b[0m corpus_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(cnt \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m chunk \u001b[38;5;28;01mfor\u001b[39;00m _, cnt \u001b[38;5;129;01min\u001b[39;00m document)\n\u001b[0;32m    846\u001b[0m subsample_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m total_docs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m--> 847\u001b[0m perwordbound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsample_ratio\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (subsample_ratio \u001b[38;5;241m*\u001b[39m corpus_words)\n\u001b[0;32m    848\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m per-word bound, \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m perplexity estimate based on a held-out corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents with \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    850\u001b[0m     perwordbound, np\u001b[38;5;241m.\u001b[39mexp2(\u001b[38;5;241m-\u001b[39mperwordbound), \u001b[38;5;28mlen\u001b[39m(chunk), corpus_words\n\u001b[0;32m    851\u001b[0m )\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perwordbound\n",
      "File \u001b[1;32md:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\models\\ldamodel.py:1113\u001b[0m, in \u001b[0;36mLdaModel.bound\u001b[1;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbound: at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, d)\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1113\u001b[0m     gammad, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1115\u001b[0m     gammad \u001b[38;5;241m=\u001b[39m gamma[d]\n",
      "File \u001b[1;32md:\\CodeProgram\\anaconda\\envs\\etm\\lib\\site-packages\\gensim\\models\\ldamodel.py:711\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpElogbeta[:, ids]\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m--> 711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpElogthetad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpElogbetad\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m epsilon\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Iterate between gamma and phi until convergence\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 分段LDA\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "file_paths = [\n",
    "        '46-55processed_text_no_stop_words.xlsx',\n",
    "        '56-65processed_text_no_stop_words.xlsx',\n",
    "        '66-75processed_text_no_stop_words.xlsx',\n",
    "        '76-85processed_text_no_stop_words.xlsx',\n",
    "        '86-95processed_text_no_stop_words.xlsx',\n",
    "        '96-05processed_text_no_stop_words.xlsx',\n",
    "        '06-15processed_text_no_stop_words.xlsx',\n",
    "        '16-23processed_text_no_stop_words.xlsx'\n",
    "    ]\n",
    "dfs = []\n",
    "texts = []\n",
    "for path in file_paths:\n",
    "    # 加载Excel文件（统一使用openpyxl引擎避免版本问题）\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "    dfs.append(df)\n",
    "    # 提取预处理文本并转换为单词列表\n",
    "    text = df['Processed_Text_No_Stop_Words'].apply(lambda x: x.split())\n",
    "    texts.append(text)\n",
    "df1_text, df2_text, df3_text, df4_text, df5_text, df6_text, df7_text, df8_text = texts\n",
    "for text in texts:\n",
    "    dictionary = Dictionary(text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in list(text)]\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=6, id2word=dictionary, passes=10)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=dictionary)\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "print(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58240758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "from gensim.models import DtmModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_paths = [\n",
    "    '46-55processed_text_no_stop_words.xlsx',\n",
    "    '56-65processed_text_no_stop_words.xlsx',\n",
    "    '66-75processed_text_no_stop_words.xlsx',\n",
    "    '76-85processed_text_no_stop_words.xlsx',\n",
    "    '86-95processed_text_no_stop_words.xlsx',\n",
    "    '96-05processed_text_no_stop_words.xlsx',\n",
    "    '06-15processed_text_no_stop_words.xlsx',\n",
    "    '16-23processed_text_no_stop_words.xlsx'\n",
    "]\n",
    "\n",
    "NUM_TOPICS = 10          # 主题数量\n",
    "MIN_WORD_FREQ = 5       # 最小词频\n",
    "MAX_WORD_RATIO = 0.5    # 最大文档占比\n",
    "ITERATIONS = 200        # 迭代次数\n",
    "PASSES = 10             # 语料遍历次数\n",
    "\n",
    "# ===================== 2. 加载并预处理数据 =====================\n",
    "time_slice_docs = []  # 所有文档的单词列表\n",
    "time_slices = []      # 每个时间切片的文档数量\n",
    "\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "    df['Processed_Text_No_Stop_Words'] = df['Processed_Text_No_Stop_Words'].fillna('')\n",
    "    \n",
    "    docs = df['Processed_Text_No_Stop_Words'].apply(lambda x: x.split()).tolist()\n",
    "    \n",
    "    time_slice_docs.extend(docs)\n",
    "    time_slices.append(len(docs))\n",
    "\n",
    "dictionary = corpora.Dictionary(time_slice_docs)\n",
    "dictionary.filter_extremes(no_below=MIN_WORD_FREQ, no_above=MAX_WORD_RATIO)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in time_slice_docs]\n",
    "\n",
    "dtm_model = DtmModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    time_slices=time_slices,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    iterations=ITERATIONS,\n",
    "    passes=PASSES,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    model=dtm_model,\n",
    "    texts=time_slice_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f274ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4043\n"
     ]
    }
   ],
   "source": [
    "#OLDA\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_paths = [\n",
    "    '46-55processed_text_no_stop_words.xlsx',\n",
    "    '56-65processed_text_no_stop_words.xlsx',\n",
    "    '66-75processed_text_no_stop_words.xlsx',\n",
    "    '76-85processed_text_no_stop_words.xlsx',\n",
    "    '86-95processed_text_no_stop_words.xlsx',\n",
    "    '96-05processed_text_no_stop_words.xlsx',\n",
    "    '06-15processed_text_no_stop_words.xlsx',\n",
    "    '16-23processed_text_no_stop_words.xlsx'\n",
    "]\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "CHUNKSIZE = 2000\n",
    "PASSES = 1\n",
    "UPDATE_EVERY = 1\n",
    "ITERATIONS = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "docs_list = []\n",
    "for path in file_paths:\n",
    "    # 加载数据+处理缺失值\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "    df['Processed_Text_No_Stop_Words'] = df['Processed_Text_No_Stop_Words'].fillna('')\n",
    "    # 转换为单词列表\n",
    "    docs = df['Processed_Text_No_Stop_Words'].apply(lambda x: x.split()).tolist()\n",
    "    docs_list.extend(docs)\n",
    "\n",
    "dictionary = corpora.Dictionary(docs_list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus_lda_c = [dictionary.doc2bow(doc) for doc in docs_list]\n",
    "total_docs = len(docs_list)\n",
    "\n",
    "olda_model = LdaModel(\n",
    "    corpus=corpus_lda_c,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    passes=PASSES,\n",
    "    update_every=UPDATE_EVERY,\n",
    "    iterations=ITERATIONS,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    model=olda_model,\n",
    "    texts=docs_list,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"{coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3514216743184268\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_paths = [\n",
    "    '46-55processed_text_no_stop_words.xlsx',\n",
    "    '56-65processed_text_no_stop_words.xlsx',\n",
    "    '66-75processed_text_no_stop_words.xlsx',\n",
    "    '76-85processed_text_no_stop_words.xlsx',\n",
    "    '86-95processed_text_no_stop_words.xlsx',\n",
    "    '96-05processed_text_no_stop_words.xlsx',\n",
    "    '06-15processed_text_no_stop_words.xlsx',\n",
    "    '16-23processed_text_no_stop_words.xlsx'\n",
    "]\n",
    "\n",
    "# OLDA核心参数\n",
    "NUM_TOPICS = 10\n",
    "CHUNKSIZE = 2000\n",
    "PASSES = 1\n",
    "UPDATE_EVERY = 1\n",
    "ITERATIONS = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "docs_by_period = []  \n",
    "all_docs_accum = [] \n",
    "\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "    df['Processed_Text_No_Stop_Words'] = df['Processed_Text_No_Stop_Words'].fillna('')\n",
    "    period_docs = df['Processed_Text_No_Stop_Words'].apply(lambda x: x.split()).tolist()\n",
    "    docs_by_period.append(period_docs)\n",
    "    all_docs_accum.extend(period_docs) \n",
    "\n",
    "first_period_docs = docs_by_period[0]\n",
    "dictionary = corpora.Dictionary(first_period_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "first_corpus = [dictionary.doc2bow(doc) for doc in first_period_docs]\n",
    "\n",
    "olda_model = LdaModel(\n",
    "    corpus=first_corpus,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    passes=PASSES,\n",
    "    update_every=UPDATE_EVERY,\n",
    "    iterations=ITERATIONS,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "coherence_1 = CoherenceModel(\n",
    "    model=olda_model,\n",
    "    texts=all_docs_accum[:len(first_period_docs)], \n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ").get_coherence()\n",
    "print(coherence_1)\n",
    "\n",
    "for i in range(1, len(docs_by_period)):\n",
    "    current_period_docs = docs_by_period[i]\n",
    "    current_period_name = i\n",
    "    current_corpus = [dictionary.doc2bow(doc) for doc in current_period_docs]\n",
    "    \n",
    "    olda_model.update(corpus=current_corpus, chunks_as_numpy=True)\n",
    "    \n",
    "    accum_docs = all_docs_accum[:sum([len(d) for d in docs_by_period[:i+1]])]\n",
    "    coherence_score = CoherenceModel(\n",
    "        model=olda_model,\n",
    "        texts=accum_docs,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    ).get_coherence()\n",
    "    \n",
    "    print(f\"{current_period_name}: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf1564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
